{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VRE_LOGLEVEL\"] = \"0\"\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import subprocess\n",
    "from omegaconf import OmegaConf\n",
    "from vre.representations import build_representations_from_cfg, ReprOut, Representation\n",
    "from vre.readers.multitask_dataset import MultiTaskDataset, MultiTaskItem\n",
    "from vre.utils import reorder_dict, collage_fn, image_add_title, get_project_root, MemoryData\n",
    "from vre_repository import get_vre_repository\n",
    "\n",
    "import numpy as np\n",
    "import torch as tr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from semantic_mapper import plot_one\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the data path.\n",
    "It can be a normal video (mp4) or a previous export of VRE if it contains the 6 relevant 'seed' experts:\n",
    "- rgb\n",
    "- opticalflow_rife\n",
    "- depth_marigold\n",
    "- normals_svd(depth_marigold)\n",
    "- semantic_mask2former_mapillary_49189528_0\n",
    "- semantic_mask2former_coco_47429163_0\n",
    "\n",
    "You can also generate it using `vre /path/to/video.mp4 -o out_dir --config_path cfg.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = Path.cwd() / \"cfg.yaml\"\n",
    "# data_path = get_project_root() / \"resources/test_video.mp4\" # It can also be a previously exported VRE dir\n",
    "data_path = Path.cwd() / \"data\"\n",
    "vre_dir = data_path\n",
    "if data_path.suffix == \".mp4\":\n",
    "    vre_dir = Path.cwd() / f\"data_{data_path.name}\"\n",
    "    frames = [\"1\", \"100\", \"1000\", \"3000\"]\n",
    "    if not vre_dir.exists():\n",
    "        subprocess.run(args=[\"vre\", str(data_path), \"--config_path\", str(cfg_path),\n",
    "                             \"-o\", str(vre_dir), \"--frames\", *frames],\n",
    "                        env={**os.environ.copy(), **{\"VRE_DEVICE\": \"cuda\" if tr.cuda.is_available() else \"cpu\"}})\n",
    "    else:\n",
    "        print(f\"Out dir '{vre_dir}' exists. Remove it first\")\n",
    "else:\n",
    "    print(\"Not a video, but a dir of exported VRE results. Skipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the MultiTaskDataset object\n",
    "\n",
    "This is only done for the initial seed expert tasks defined earlier. We'll also plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_names = [\"rgb\", \"depth_marigold\", \"normals_svd(depth_marigold)\", \"opticalflow_rife\",\n",
    "              \"semantic_mask2former_coco_47429163_0\", \"semantic_mask2former_mapillary_49189528_0\"]\n",
    "order = [\"rgb\", \"semantic_mask2former_mapillary_49189528_0\", \"semantic_mask2former_coco_47429163_0\",\n",
    "            \"depth_marigold\", \"normals_svd(depth_marigold)\"]\n",
    "\n",
    "representations = build_representations_from_cfg(cfg_path, get_vre_repository())\n",
    "name_to_repr = {r.name: r for r in representations}\n",
    "reader = MultiTaskDataset(vre_dir, task_names=task_names,\n",
    "                          task_types=name_to_repr, handle_missing_data=\"fill_nan\",\n",
    "                          normalization=\"min_max\", cache_task_stats=True, batch_size_stats=100)\n",
    "orig_task_names = list(reader.task_types.keys())\n",
    "\n",
    "print(reader)\n",
    "print(\"== Shapes ==\")\n",
    "pprint(reader.data_shape)\n",
    "\n",
    "data, name = reader[np.random.randint(0, len(reader))]\n",
    "collage = plot_one(data, title=name, order=order, name_to_task=reader.name_to_task)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.imshow(collage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the newly defined tasks\n",
    "\n",
    "Thery are read from `semantic_mapper.py` but you can define new ones here too using the primitives from there.\n",
    "Each of them is added to the MultiTaskDataset object one by one.\n",
    "\n",
    "The computation in the reader is done on the fly, meaning they are derived from the underlying existing experts\n",
    "on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_mapper import get_new_semantic_mapped_tasks # put this here for auto loading\n",
    "new_tasks = get_new_semantic_mapped_tasks(include_semantic_output=False)\n",
    "for task_name in reader.task_names:\n",
    "    if task_name not in orig_task_names:\n",
    "        reader.remove_task(task_name)\n",
    "for new_task in new_tasks.values():\n",
    "    reader.add_task(new_task, overwrite=True)\n",
    "\n",
    "print(\"== Random loaded item ==\")\n",
    "ixs = np.random.permutation(range(len(reader))).tolist()\n",
    "for ix in ixs:\n",
    "    data, name = reader[ix]\n",
    "    print(data)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(collage := plot_one(data, title=name, order=order, name_to_task=reader.name_to_task))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
