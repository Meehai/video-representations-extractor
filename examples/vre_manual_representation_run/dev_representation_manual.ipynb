{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as tr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "\n",
    "from vre.representations import build_representations_from_cfg\n",
    "from vre.utils import get_project_root, vre_yaml_load, collage_fn, image_write\n",
    "from vre_video import VREVideo\n",
    "from vre_repository import get_vre_repository\n",
    "from vre_repository.optical_flow.raft import FlowRaft\n",
    "\n",
    "device = \"cuda\" if tr.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ctx:\n",
    "    def __init__(self, _repr):\n",
    "        self.repr = _repr\n",
    "    def __enter__(self):\n",
    "        self.repr.vre_setup() if self.repr.setup_called is False else None\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.repr.vre_free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = VREVideo(get_project_root() / \"resources/test_video.mp4\")\n",
    "print(video.shape, video.fps)\n",
    "h, w = video.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VRE_DEVICE\"] = device = \"cuda\" if tr.cuda.is_available() else \"cpu\"\n",
    "all_representations_dict = vre_yaml_load(Path.cwd() / \"cfg.yaml\")\n",
    "device = \"cuda\" if tr.cuda.is_available() else \"cpu\"\n",
    "representations = build_representations_from_cfg(all_representations_dict, representation_types=get_vre_repository())\n",
    "name_to_repr = {r.name: r for r in representations}\n",
    "print(representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the representations for two particular frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference setup (this is done inside VRE's main loop at run() as well)\n",
    "depth, normals = name_to_repr[\"depth_marigold\"], name_to_repr[\"normals_svd(depth_marigold)\"]\n",
    "depth2, normals2 = name_to_repr[\"depth_dpt\"], name_to_repr[\"normals_svd(depth_dpt)\"]\n",
    "\n",
    "np.random.seed(43)\n",
    "mb = 1\n",
    "ixs = sorted([np.random.randint(0, len(video) - 1) for _ in range(mb)])\n",
    "print(ixs)\n",
    "\n",
    "with Ctx(depth):\n",
    "    y_depth_img = depth.make_images(out_depth := depth.resize(depth.compute(video, ixs), (h, w)))\n",
    "y_normals_img = normals.make_images(out_normals := normals.compute(video, ixs, [out_depth]))\n",
    "with Ctx(depth2):\n",
    "    y_depth2_img = depth2.make_images(out_depth := depth2.resize(depth2.compute(video, ixs), (h, w)))\n",
    "y_normals2_img = normals2.make_images(out_normals := normals2.compute(video, ixs, [out_depth]))\n",
    "\n",
    "for i in range(mb):\n",
    "    titles = [\"RGB\", \"Depth Marigold\", \"Normals SVD (Depth Marigold)\", \"\", \"Depth DPT\", \"Normals SVD (Depth DPT)\"]\n",
    "    imgs = [out_depth.frames[i], y_depth_img[i], y_normals_img[i], y_depth_img[i]*0, y_depth2_img[i], y_normals2_img[i]]\n",
    "    img = collage_fn(imgs, titles=titles, rows_cols=(2, 3))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical flow +/-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = video.shape[1:3]\n",
    "# h, w = [540, 960]\n",
    "print(h, w)\n",
    "flow = FlowRaft(name=\"flow_raft\", dependencies=[], inference_width=w, inference_height=h, iters=5, small=False, delta=5)\n",
    "flow_l = FlowRaft(name=\"flow_raft\", dependencies=[], inference_width=w, inference_height=h, iters=5,\n",
    "                small=False, delta=-5)\n",
    "flow.device = flow_l.device = device\n",
    "\n",
    "# np.random.seed(43)\n",
    "mb = 2\n",
    "ixs = sorted([np.random.randint(0, len(video) - 1) for _ in range(mb)])\n",
    "print(ixs)\n",
    "\n",
    "with Ctx(flow):\n",
    "    y_flow = flow.compute(video, ixs)\n",
    "with Ctx(flow_l):\n",
    "    y_flow_l = flow_l.compute(video, ixs)\n",
    "print(y_flow.output.reshape(-1, 2).mean(0) * [h, w], y_flow.output.reshape(-1, 2).std(0))\n",
    "print(y_flow_l.output.reshape(-1, 2).mean(0)  * [h, w], y_flow_l.output.reshape(-1, 2).std(0))\n",
    "flow_img = flow.make_images(y_flow)\n",
    "flow_l_img = flow_l.make_images(y_flow_l)\n",
    "for i in range(mb):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "    ax[0].imshow(video[ixs[i]])\n",
    "    ax[1].imshow(flow_img[i])\n",
    "    ax[2].imshow(flow_l_img[i])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
